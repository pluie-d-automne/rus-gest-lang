# Computer vision

# ANNs
|Тип сети|Особенности|Какие задачи решает|Примеры|
|---|---|---|---|
|MLP (multilayer perceptron)||||
|CNN|||[LeNet](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf), [AlexNet](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf), [VGGNet](https://arxiv.org/pdf/1409.1556.pdf), [Inception module and GoogLeNet](https://arxiv.org/pdf/1409.4842v1.pdf), [ResNet](https://arxiv.org/pdf/1512.03385.pdf)|
|||||
|||||

# Определения и понятия
**Изображение как функция**:
* чёрно-белые - F(x, y) = 255 (255-белый пиксел в точке с координатами x и y)
* цветные - F(x,y) = (r, g, b) (точка с координатами x и y описывается как вектор с интенсивностями для трёх каналов - красный, зелёный, голубой)

**Предобработка изображений**
* Преобразование цветных изображений в чёрно-белые (снижает сложность вычислений, но если цвет важен, то потерятся важная информация)
* Стандартизация изображений - приведение их к единому размеру (важно для некоторых алгоритмов, например, CNN)
* Аугментация данных - создание новых изображений путём модификации старых (поворот, изменение масштаба и т.п.)

**Пайплайн классификации**:\
картинка на входе -> предобработка картинки -> извлечение фичей -> классификация

**Основные компоненты нейросети**
* input layer - вектор фичей на фходе
* hidden layers - все нейроны межды входом и выходом
* weight connections (edges) - связи между нейронами - "веса", с которыми один нейрон влияет на другой
* output layer - искомое значение (в задачах регрессии) или вектор вероятностей (в задачах классификации)

## Функции активации
1) Step function
```python
def step_function(x):
    if x <= 0:
        return 0
    else:
        return 1
```
2) Sigmoid/logistic function\
$\sigma(z)={1\over(1+e^{-z})}$

```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))
```
3) Softmax function\
Обобщение sigmoid (для случая более двух классов). Часто используется в output layer в задачах классификации.\
$\sigma(x_j)={{e^{x_j}}\over(\sum_i1+e^{xi})}$

4) Hyperbolic tangent function (tanh)\
${tanh(x)}={{sinh(x)}\over{cosh(x)}}={{e^x-e^{-x}}\over{e^x+e^{-x}}}$\
Часто работает лучше сигмоида на скрытых слоях. Так как центр в 0, а не 0.5 (несколько проще обучение).

5) ReLU (rectified linear unit)\
$f(x) = max(0, x)$\
Используется чаще всего на скрытых слоях.

6) Leaky ReLU\
$f(x) = max(0.01x, x)$