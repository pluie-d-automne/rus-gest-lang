{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G3-oihH2rI-"
      },
      "source": [
        "https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "\n",
        "A custom Dataset class must implement three functions: __init__, __len__, and __getitem__.\n",
        "\n",
        "https://github.com/bomri/SlowFast/blob/master/slowfast/datasets/loader.py\n",
        "\n",
        "https://github.com/bomri/SlowFast/blob/master/slowfast/datasets/ava_dataset.py\n",
        "\n",
        "https://github.com/HHTseng/video-classification/blob/master/ResNetCRNN_varylength/UCF101_ResNetCRNN_varlen.py\n",
        "https://www.ai-contentlab.com/2023/01/video-classification-is-important-task.html\n",
        "\n",
        "https://discuss.pytorch.org/t/how-upload-sequence-of-image-on-video-classification/24865/13\n",
        "\n",
        "Оптический поток\n",
        "https://docs.opencv.org/2.4/modules/video/doc/motion_analysis_and_object_tracking.html\n",
        "\n",
        "Skeleton\n",
        "https://www.fireblazeaischool.in/blogs/human-pose-estimation-using-opencv/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loader\n",
        "\n",
        "Добавить нормализацию!!!"
      ],
      "metadata": {
        "id": "sL0Mup8ev7iQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_list = ['пингвин',\n",
        " 'жираф',\n",
        " 'лягушка',\n",
        " 'бегемот',\n",
        " 'козел',\n",
        " 'лиса',\n",
        " 'динозавр',\n",
        " 'кролик',\n",
        " 'собака',\n",
        " 'обезьяна',\n",
        " 'корова',\n",
        " 'свинья',\n",
        " 'медуза',\n",
        " 'курица',\n",
        " 'павлин',\n",
        " 'дельфин',\n",
        " 'слон',\n",
        " 'медведь',\n",
        " 'лебедь',\n",
        " 'орел',\n",
        " 'бык',\n",
        " 'змея',\n",
        " 'птица',\n",
        " 'лось',\n",
        " 'пчела',\n",
        " 'лев',\n",
        " 'тигр',\n",
        " 'мышь',\n",
        " 'паук',\n",
        " 'бабочка']\n",
        "\n",
        " # Курс Седжвика по алгоритмам"
      ],
      "metadata": {
        "id": "_a8oa_eOWDoS"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "sIrdshw322FY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision\n",
        "#from tensorflow.keras.layers import Concatenate\n",
        "class BasicVideoDataset(Dataset):\n",
        "    def __init__(self, annotations_file, video_dir, IMG_SIZE, labels_list):\n",
        "        self.video_labels = pd.read_csv(annotations_file, sep='\\t')\n",
        "        self.video_dir = video_dir\n",
        "        self.IMG_SIZE = IMG_SIZE\n",
        "        self.frames_cnt = max(self.video_labels['end']-self.video_labels['begin'])\n",
        "        self.labels_list = labels_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_labels)\n",
        "\n",
        "    def crop_center_square(self, frame):\n",
        "        y, x = frame.shape[0:2]\n",
        "        min_dim = min(y, x)\n",
        "        start_x = (x // 2) - (min_dim // 2)\n",
        "        start_y = (y // 2) - (min_dim // 2)\n",
        "        return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
        "\n",
        "\n",
        "    def load_video(self, path, begin, end, max_frames=0, resize=(10, 10)):\n",
        "        cap = cv2.VideoCapture(path)\n",
        "        frames = []\n",
        "\n",
        "        frame_index=begin\n",
        "        try:\n",
        "            while True and frame_index <= end:\n",
        "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
        "                ret, frame = cap.read()\n",
        "                if not ret:\n",
        "                    break\n",
        "                frame = self.crop_center_square(frame)\n",
        "                frame = cv2.resize(frame, resize)\n",
        "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # convert to grayscale\n",
        "                #frame = Concatenate()([frame, frame, frame])\n",
        "                #frame = np.dstack((frame, frame, frame))\n",
        "                frame = np.array([frame, frame, frame])\n",
        "                frames.append(frame)\n",
        "                frame_index+=1\n",
        "\n",
        "                if len(frames) == max_frames:\n",
        "                    break\n",
        "        finally:\n",
        "            cap.release()\n",
        "        return torch.from_numpy(np.array(frames))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        filename  = os.path.join(self.video_dir, self.video_labels.iloc[idx]['attachment_id']+\".mp4\")\n",
        "        label = self.video_labels.iloc[idx]['text']\n",
        "        begin = self.video_labels.iloc[idx]['begin']\n",
        "        end = self.video_labels.iloc[idx]['end']\n",
        "        frames = self.load_video(filename, begin, end, resize=(self.IMG_SIZE, self.IMG_SIZE)) # Загрузка видео!!!!\n",
        "        return frames/255, labels_list.index(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVrvMnpp5OjJ"
      },
      "source": [
        "* The __init__ function is run once when instantiating the Dataset object. We initialize the directory containing the images, the annotations file, and both transforms (covered in more detail in the next section).\n",
        "* The __len__ function returns the number of samples in our dataset.\n",
        "* The __getitem__ function loads and returns a sample from the dataset at the given index idx."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "YV2k0jav3Ygd"
      },
      "outputs": [],
      "source": [
        "annotations_file = \"/content/SLOVO_DF_SHORT.tsv\"\n",
        "video_dir = \"/content/videos\"\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 1\n",
        "LEARNING_RATE = 0.0001\n",
        "NUM_EPOCHS = 10\n",
        "num_classes = 30\n",
        "model_type = 'rnn'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "qxL6umLs3sf6"
      },
      "outputs": [],
      "source": [
        "training_data = BasicVideoDataset(annotations_file, video_dir, IMG_SIZE=IMG_SIZE, labels_list=labels_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "hnSV_-Ne1546"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "frames, label = next(iter(train_dataloader))"
      ],
      "metadata": {
        "id": "2tXhCmMqGyzE"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frames.shape\n",
        "# 1,x,3,244,244"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcDbpBd-MPaj",
        "outputId": "3b3dfd8e-eb43-4064-ed82-79d13570dcc7"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 48, 3, 224, 224])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0hlodXXZFuT",
        "outputId": "86b95333-cd84-4a05-9fdd-b827a05f34b5"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "frames.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hi5CjrTn4Ubj",
        "outputId": "68d44909-642a-427a-e5e7-bca0a95eef15"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label.dtype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBzp4HYL4Wlz",
        "outputId": "14f94d53-875e-479e-a1e3-4f46e6676c9f"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.int64"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "EX1m2LKowAuk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://programming.vip/docs/pytorch-basics-14-video-classification-based-on-pytorch.html"
      ],
      "metadata": {
        "id": "0HiYtNyjbNtb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to be able to train our model on a hardware accelerator like the GPU or MPS, if available. Let’s check to see if torch.cuda or torch.backends.mps are available, otherwise we use the CPU."
      ],
      "metadata": {
        "id": "Nr7LRDHRv55x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "#device = \"cuda\""
      ],
      "metadata": {
        "id": "RsWcBZEhvtCc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e91016f-8506-491e-8bea-0028cc7bf41b"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method."
      ],
      "metadata": {
        "id": "KHVNfZyewF4d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_type==\"rnn\"\n",
        "\n",
        "class Resnet18Rnn(nn.Module):\n",
        "\tdef __init__(self,params_model):\n",
        "\t\tsuper(Resnet18Rnn,self).__init__()\n",
        "\t\tnum_classes=params_model[\"num_classes\"]\n",
        "\t\tdr_rate=params_model[\"dr_rate\"]\n",
        "\t\tpretrained=params_model[\"pretrained\"]\n",
        "\t\trnn_hidden_size=params_model[\"rnn_hidden_size\"]\n",
        "\t\trnn_num_layers=params_model[\"rnn_num_layers\"]\n",
        "\t\tbaseModel=torchvision.models.resnet18(pretrained=pretrained)\n",
        "\t\tnum_features=baseModel.fc.in_features\n",
        "    # baseModel.classifier[-1]=Identity()\n",
        "\t\tbaseModel.fc=Identity() # обнуляем fully connected layer\n",
        "\t\tself.baseModel=baseModel\n",
        "\t\tself.dropout=nn.Dropout(dr_rate)\n",
        "\t\tself.rnn=nn.LSTM(num_features,rnn_hidden_size,rnn_num_layers)\n",
        "\t\tself.fc1=nn.Linear(rnn_hidden_size, num_classes)\n",
        "\tdef forward(self,x):\n",
        "\t\ttry:\n",
        "\t\t\t\tb_z,ts,c,h,w=x.shape\n",
        "\t\t\t\tii=0\n",
        "\t\t\t\ty=self.baseModel((x[:,ii]))\n",
        "\t\t\t\tout,(hn,cn)=self.rnn(y.unsqueeze(1))\n",
        "\t\t\t\tfor ii in range(1,ts):\n",
        "\t\t\t\t\ty=self.baseModel((x[:,ii]))\n",
        "\t\t\t\t\tout,(hn,cn)=self.rnn(y.unsqueeze(1),(hn,cn))\n",
        "\t\t\t\tout=self.dropout(out[:,-1])\n",
        "\t\t\t\tout=self.fc1(out)\n",
        "\t\texcept:\n",
        "\t\t\t\tprint(f'x: {x}')\n",
        "\t\t\t\tprint(f'x.shape: {x.shape}')\n",
        "\t\t\t\traise\n",
        "\t\treturn out\n",
        "\n",
        "class Identity(nn.Module):\n",
        "\tdef __init__(self):\n",
        "\t\tsuper(Identity,self).__init__()\n",
        "\tdef forward(self,x):\n",
        "\t\treturn x\n",
        "\n",
        "params_model={\n",
        "\t\t\"num_classes\":num_classes,\n",
        "\t\t\"dr_rate\":0.1,\n",
        "\t\t\"pretrained\":True,\n",
        "\t\t\"rnn_num_layers\":1,\n",
        "\t\t\"rnn_hidden_size\":100,\n",
        "\t\t}\n",
        "model=Resnet18Rnn(params_model)\n",
        "\n",
        "#3. Use some virtual input to test the model\n",
        "with torch.no_grad():\n",
        "\tif model_type==\"rnn\":\n",
        "\t\tx=torch.zeros(1,16,3,244,244)\n",
        "\telse:\n",
        "\t\tx=torch.zeros(1,3,16,244,244)\n",
        "\ty = model(x)\n",
        "\tprint(y.shape)\n",
        "\n",
        "#4. Move the model to the GPU device\n",
        "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model=model.to(device)\n",
        "\n",
        "#5. Print model\n",
        "print(model)\n",
        "\n",
        "# According to model_type, the corresponding model will be printed. The following is the result of printing 3dcnn model:\n",
        "# VideoResNet(\n",
        "# (stem): BasicStem(\n",
        "# (0): Conv3d(3, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n",
        "# (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "# (2): ReLU(inplace=True)\n",
        "# )\n",
        "# ...\n",
        "# The printing results of rnn model are as follows:\n",
        "# Resnt18Rnn(\n",
        "# (baseModel): ResNet(\n",
        "# (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "# (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "# (relu): ReLU(inplace=True)\n",
        "# (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
        "# ...\n"
      ],
      "metadata": {
        "id": "T9VvWB81wGiQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fb95f9f-d291-4422-ebe0-2b7557af439e"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 30])\n",
            "Resnet18Rnn(\n",
            "  (baseModel): ResNet(\n",
            "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "    (layer1): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (layer4): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (downsample): Sequential(\n",
            "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (relu): ReLU(inplace=True)\n",
            "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "    (fc): Identity()\n",
            "  )\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (rnn): LSTM(512, 100)\n",
            "  (fc1): Linear(in_features=100, out_features=30, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create an instance of NeuralNetwork, and move it to the device, and print its structure."
      ],
      "metadata": {
        "id": "6bXnHS7awQLP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use the model, we pass it the input data. This executes the model’s forward, along with some background operations. Do not call model.forward() directly!\n",
        "\n",
        "Calling the model on the input returns a 2-dimensional tensor with dim=0 corresponding to each output of 10 raw predicted values for each class, and dim=1 corresponding to the individual values of each output. We get the prediction probabilities by passing it through an instance of the nn.Softmax module."
      ],
      "metadata": {
        "id": "m5EtY3yGw36S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Обучение"
      ],
      "metadata": {
        "id": "ddoo_ipAXFNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pylab as plt\n",
        "import copy\n",
        "\n",
        "def get_lr(opt):\n",
        "\tfor param_group in opt.param_groups:\n",
        "\t\treturn param_group[\"lr\"]\n",
        "\n",
        "def metrics_batch(output, target):\n",
        "\tpred=output.argmax(dim=1,keepdim=True)\n",
        "\tcorrects=pred.eq(target.view_as(pred)).sum().item()\n",
        "\treturn corrects\n",
        "\n",
        "def loss_batch(loss_func, output, target, opt=None):\n",
        "\tloss=loss_func(output, target)\n",
        "\twith torch.no_grad():\n",
        "\t\tmetric_b=metrics_batch(output,target)\n",
        "\tif opt is not None:\n",
        "\t\topt.zero_grad()\n",
        "\t\tloss.backward()\n",
        "\t\topt.step()\n",
        "\treturn loss.item(), metric_b\n",
        "\n",
        "def loss_epoch(model, loss_func, dataset_dl, sanity_check=False,opt=None):\n",
        "\trunning_loss=0.0\n",
        "\trunning_metric=0.0\n",
        "\tlen_data=len(dataset_dl.dataset)\n",
        "\tfor xb,yb in dataset_dl:\n",
        "\t\txb=xb.to(device)\n",
        "\t\tyb=yb.to(device)\n",
        "\t\toutput=model(xb)\n",
        "\t\tloss_b,metric_b=loss_batch(loss_func,output,yb,opt)\n",
        "\t\trunning_loss+=loss_b\n",
        "\t\tif metric_b is not None:\n",
        "\t\t\trunning_metric+=metric_b\n",
        "\t\tif sanity_check is True:\n",
        "\t\t\tbreak\n",
        "\tloss=running_loss/float(len_data)\n",
        "\tmetric=running_metric/float(len_data)\n",
        "\treturn loss, metric\n",
        "\n",
        "def plot_loss(loss_hist, metric_hist):\n",
        "\tnum_epochs=len(loss_hist[\"train\"])\n",
        "\tplt.title(\"Train-Val Loss\")\n",
        "\tplt.plot(range(1,num_epochs+1),loss_hist[\"train\"],label=\"train\")\n",
        "\tplt.plot(range(1,num_epochs+1),loss_hist[\"val\"],label=\"val\")\n",
        "\tplt.ylabel(\"Loss\")\n",
        "\tplt.xlabel(\"Training Epochs\")\n",
        "\tplt.legend()\n",
        "\tplt.show()\n",
        "\tplt.title(\"Train-Val Accuracy\")\n",
        "\tplt.plot(range(1,num_epochs+1),metric_hist[\"train\"],label=\"train\")\n",
        "\tplt.plot(range(1,num_epochs+1),metric_hist[\"val\"],label=\"val\")\n",
        "\tplt.ylabel(\"Accuracy\")\n",
        "\tplt.xlabel(\"Training Epochs\")\n",
        "\tplt.legend()\n",
        "\tplt.show()\n",
        "\n",
        "def train_val(model, params):\n",
        "\tnum_epochs=params[\"num_epochs\"]\n",
        "\tloss_func=params[\"loss_func\"]\n",
        "\topt=params[\"optimizer\"]\n",
        "\ttrain_dl=params[\"train_dl\"]\n",
        "\tval_dl=params[\"val_dl\"]\n",
        "\tsanity_check=params[\"sanity_check\"]\n",
        "\tlr_scheduler=params[\"lr_scheduler\"]\n",
        "\tpath2weights=params[\"path2weights\"]\n",
        "\n",
        "\tloss_history={\"train\":[],\"val\":[]}\n",
        "\tmetric_history={\"train\":[],\"val\":[]}\n",
        "\tbest_model_wts=copy.deepcopy(model.state_dict())\n",
        "\tbest_loss=float(\"inf\")\n",
        "\tfor epoch in range(num_epochs):\n",
        "\t\tcurrent_lr=get_lr(opt)\n",
        "\t\tprint(\"Epoch {}/{}, current lr={}\".format(epoch, num_epochs-1,current_lr))\n",
        "\t\tmodel.train()\n",
        "\t\ttrain_loss,train_metric = loss_epoch(model, loss_func, train_dl, sanity_check,opt)\n",
        "\t\tloss_history[\"train\"].append(train_loss)\n",
        "\t\tmetric_history[\"train\"].append(train_metric)\n",
        "\t\tmodel.eval()\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tval_loss, val_metric = loss_epoch(model,loss_func,val_dl,sanity_check)\n",
        "\t\tif val_loss<best_loss:\n",
        "\t\t\tbest_loss=val_loss\n",
        "\t\t\tbest_model_wts=copy.deepcopy(model.state_dict())\n",
        "\t\t\ttorch.save(model.state_dict(),path2weights)\n",
        "\t\t\tprint(\"Copied best model weights\")\n",
        "\t\tloss_history[\"val\"].append(val_loss)\n",
        "\t\tmetric_history[\"val\"].append(val_metric)\n",
        "\t\tlr_scheduler.step(val_loss)\n",
        "\t\tif current_lr!=get_lr(opt):\n",
        "\t\t\tprint(\"Loading best model weights\")\n",
        "\t\t\tmodel.load_state_dict(best_model_wts)\n",
        "\t\tprint(\"Train loss:%.6f, dev loss:%.6f, accuracy:%.2f\" % (train_loss, val_loss, 100*val_metric))\n",
        "\t\tprint(\"-\"*10)\n",
        "\tmodel.load_state_dict(best_model_wts)\n",
        "\treturn model, loss_history, metric_history"
      ],
      "metadata": {
        "id": "42djnzQ62Ngi"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(device)\n",
        "# criterion = nn.CrossEntropyLoss().to(device)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "hoSYMT2OXHO3"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
        "loss_func=nn.CrossEntropyLoss(reduction=\"sum\")\n",
        "opt=torch.optim.Adam(model.parameters(),lr=3e-5)\n",
        "# The change of LR in cosine annealing learning rate is periodic, T_max is 1 / 2 of the period; eta_min(float) represents the minimum learning rate, which is 0 by default;\n",
        "# last_epoch(int) represents the number of previous epoch, which is used to indicate whether the learning rate needs to be adjusted. When last_ When the epoch meets the set interval,\n",
        "# The learning rate will be adjusted. When - 1, the learning rate is set to the initial value.\n",
        "# lr_scheduler = CosineAnnealingLR(opt, T_max=20, verbose=True)\n",
        "lr_scheduler=ReduceLROnPlateau(opt,mode=\"min\",factor=0.5,patience=5,verbose=1)\n",
        "os.makedirs(\"./models\",exist_ok=True)\n",
        "#2. Call train in myutils_ Val auxiliary function training model\n",
        "params_train={\n",
        "\t\"num_epochs\":20,\n",
        "\t\"optimizer\":opt,\n",
        "\t\"loss_func\":loss_func,\n",
        "\t\"train_dl\":train_dataloader,\n",
        "\t\"val_dl\":train_dataloader,\n",
        "\t\"sanity_check\":True,\n",
        "\t\"lr_scheduler\":lr_scheduler,\n",
        "\t\"path2weights\":\"./models/weights_\"+model_type+\".pt\",\n",
        "}\n",
        "model,loss_hist,metric_hist=train_val(model,params_train)\n",
        "# After running the previous code snippet, the training will begin, and you should see its progress on the screen.\n",
        "#3. After the training, draw the training progress\n",
        "plot_loss(loss_hist, metric_hist)\n",
        "# The previous clip will show a graph of loss and accuracy."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 852
        },
        "id": "UyHmxxUB1hpE",
        "outputId": "9496b6c9-5d2e-4966-c131-bc8a90c10ca5"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/19, current lr=3e-05\n",
            "Copied best model weights\n",
            "Train loss:0.005652, dev loss:0.005698, accuracy:0.00\n",
            "----------\n",
            "Epoch 1/19, current lr=3e-05\n",
            "Train loss:0.005844, dev loss:0.005877, accuracy:0.00\n",
            "----------\n",
            "Epoch 2/19, current lr=3e-05\n",
            "Copied best model weights\n",
            "Train loss:0.005932, dev loss:0.005531, accuracy:0.00\n",
            "----------\n",
            "Epoch 3/19, current lr=3e-05\n",
            "Train loss:0.005620, dev loss:0.006366, accuracy:0.00\n",
            "----------\n",
            "Epoch 4/19, current lr=3e-05\n",
            "Copied best model weights\n",
            "Train loss:0.005151, dev loss:0.005468, accuracy:0.00\n",
            "----------\n",
            "Epoch 5/19, current lr=3e-05\n",
            "Train loss:0.006052, dev loss:0.005909, accuracy:0.00\n",
            "----------\n",
            "Epoch 6/19, current lr=3e-05\n",
            "x: tensor([], size=(1, 0), dtype=torch.float64)\n",
            "x.shape: torch.Size([1, 0])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-30885dd9456d>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;34m\"path2weights\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"./models/weights_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".pt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m }\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_hist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetric_hist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m# After running the previous code snippet, the training will begin, and you should see its progress on the screen.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#3. After the training, draw the training progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-80-ff5685c747ba>\u001b[0m in \u001b[0;36mtrain_val\u001b[0;34m(model, params)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch {}/{}, current lr={}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurrent_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msanity_check\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m                 \u001b[0mloss_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0mmetric_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_metric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-80-ff5685c747ba>\u001b[0m in \u001b[0;36mloss_epoch\u001b[0;34m(model, loss_func, dataset_dl, sanity_check, opt)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0myb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                 \u001b[0mloss_b\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetric_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0mrunning_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-79-98e2803e7302>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                                 \u001b[0mb_z\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                                 \u001b[0mii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                                 \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbaseModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "p9C0zxcrOz76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.eval()\n",
        "# with torch.no_grad():\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "#     for i, (frames, labels) in enumerate(train_dataloader):\n",
        "#       frames = frames.to(device, dtype=torch.float)\n",
        "#       labels = labels.to(device, dtype=torch.float)\n",
        "#       outputs = model(frames)\n",
        "#       _, predicted = torch.max(outputs.data, 1)\n",
        "#       total += labels.size(0)\n",
        "#       correct += (predicted == labels).sum().item()\n",
        "\n",
        "# print('Test Accuracy of the model on the 10000 test images: {} %'.format((correct / total) * 100))\n",
        "\n"
      ],
      "metadata": {
        "id": "5SEVU34dhWbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Сохраняем модель и строим график\n",
        "# torch.save(model.state_dict(), MODEL_STORE_PATH + 'conv_net_model.ckpt')"
      ],
      "metadata": {
        "id": "li8mPAjQk0iG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# images.dtype"
      ],
      "metadata": {
        "id": "Nqpefp4sk5J7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}