{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1G3-oihH2rI-"
   },
   "source": [
    "https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "A custom Dataset class must implement three functions: __init__, __len__, and __getitem__.\n",
    "\n",
    "https://github.com/bomri/SlowFast/blob/master/slowfast/datasets/loader.py\n",
    "\n",
    "https://github.com/bomri/SlowFast/blob/master/slowfast/datasets/ava_dataset.py\n",
    "\n",
    "https://github.com/HHTseng/video-classification/blob/master/ResNetCRNN_varylength/UCF101_ResNetCRNN_varlen.py\n",
    "https://www.ai-contentlab.com/2023/01/video-classification-is-important-task.html\n",
    "\n",
    "https://discuss.pytorch.org/t/how-upload-sequence-of-image-on-video-classification/24865/13\n",
    "\n",
    "Оптический поток\n",
    "https://docs.opencv.org/2.4/modules/video/doc/motion_analysis_and_object_tracking.html\n",
    "\n",
    "Skeleton\n",
    "https://www.fireblazeaischool.in/blogs/human-pose-estimation-using-opencv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sL0Mup8ev7iQ"
   },
   "source": [
    "# Data Loader\n",
    "\n",
    "Добавить нормализацию!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T17:39:49.230086Z",
     "iopub.status.busy": "2023-11-21T17:39:49.229322Z",
     "iopub.status.idle": "2023-11-21T17:39:49.246335Z",
     "shell.execute_reply": "2023-11-21T17:39:49.245693Z",
     "shell.execute_reply.started": "2023-11-21T17:39:49.230054Z"
    },
    "id": "sIrdshw322FY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasicVideoDataset(Dataset):\n",
    "    def __init__(self, labels_list, video_dir, IMG_SIZE, labels_df):\n",
    "        self.video_labels = labels_df\n",
    "        self.video_dir = video_dir\n",
    "        self.IMG_SIZE = IMG_SIZE\n",
    "        self.frames_cnt = max(self.video_labels['end']-self.video_labels['begin'])\n",
    "        self.labels_list = labels_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_labels)\n",
    "\n",
    "    def crop_center_square(self, frame):\n",
    "        y, x = frame.shape[0:2]\n",
    "        min_dim = min(y, x)\n",
    "        start_x = (x // 2) - (min_dim // 2)\n",
    "        start_y = (y // 2) - (min_dim // 2)\n",
    "        return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "    def load_video(self, path, begin, end, max_frames=0, resize=(10, 10)):\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        frames = []\n",
    "\n",
    "        frame_index=begin\n",
    "        try:\n",
    "            while True and frame_index <= end:\n",
    "                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                frame = self.crop_center_square(frame)\n",
    "                frame = cv2.resize(frame, resize)\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # convert to grayscale\n",
    "                #frame = Concatenate()([frame, frame, frame])\n",
    "                #frame = np.dstack((frame, frame, frame))\n",
    "                frame = np.array([frame, frame, frame])\n",
    "                frames.append(frame)\n",
    "                frame_index+=1\n",
    "\n",
    "                if len(frames) == max_frames:\n",
    "                    break\n",
    "        finally:\n",
    "            cap.release()\n",
    "        return torch.from_numpy(np.array(frames))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename  = os.path.join(self.video_dir, self.video_labels.iloc[idx]['attachment_id']+\".mp4\")\n",
    "        label = self.video_labels.iloc[idx]['text']\n",
    "        begin = self.video_labels.iloc[idx]['begin']\n",
    "        end = self.video_labels.iloc[idx]['end']\n",
    "        frames = self.load_video(filename, begin, end, resize=(self.IMG_SIZE, self.IMG_SIZE)) # Загрузка видео!!!!\n",
    "        return frames/255, labels_list.index(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVrvMnpp5OjJ"
   },
   "source": [
    "* The __init__ function is run once when instantiating the Dataset object. We initialize the directory containing the images, the annotations file, and both transforms (covered in more detail in the next section).\n",
    "* The __len__ function returns the number of samples in our dataset.\n",
    "* The __getitem__ function loads and returns a sample from the dataset at the given index idx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T17:40:38.854447Z",
     "iopub.status.busy": "2023-11-21T17:40:38.853771Z",
     "iopub.status.idle": "2023-11-21T17:40:38.869901Z",
     "shell.execute_reply": "2023-11-21T17:40:38.869280Z",
     "shell.execute_reply.started": "2023-11-21T17:40:38.854412Z"
    },
    "id": "YV2k0jav3Ygd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "annotations_file = \"/home/jupyter/mnt/s3/rsl-videos/slovo/slovo_annotations/SLOVO_DATAFRAME.tsv\"\n",
    "video_dir = \"/home/jupyter/mnt/s3/rsl-videos/slovo/slovo\"\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_EPOCHS = 10\n",
    "model_type = 'rnn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T17:31:08.641042Z",
     "iopub.status.busy": "2023-11-21T17:31:08.640367Z",
     "iopub.status.idle": "2023-11-21T17:31:08.879936Z",
     "shell.execute_reply": "2023-11-21T17:31:08.879194Z",
     "shell.execute_reply.started": "2023-11-21T17:31:08.641001Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attachment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>length</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>group_rank</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44e8d2a0-7e01-450b-90b0-beb7400d2c1e</td>\n",
       "      <td>Ё</td>\n",
       "      <td>185bd3a81d9d618518d10abebf0d17a8</td>\n",
       "      <td>640</td>\n",
       "      <td>360</td>\n",
       "      <td>156.0</td>\n",
       "      <td>36</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>df5b08f0-41d1-4572-889c-8b893e71069b</td>\n",
       "      <td>А</td>\n",
       "      <td>185bd3a81d9d618518d10abebf0d17a8</td>\n",
       "      <td>640</td>\n",
       "      <td>360</td>\n",
       "      <td>150.0</td>\n",
       "      <td>36</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17f53df4-c467-4aff-9f48-20687b63d49a</td>\n",
       "      <td>Р</td>\n",
       "      <td>185bd3a81d9d618518d10abebf0d17a8</td>\n",
       "      <td>640</td>\n",
       "      <td>360</td>\n",
       "      <td>133.0</td>\n",
       "      <td>40</td>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e3add916-c708-4339-ad98-7e2740be29e9</td>\n",
       "      <td>Е</td>\n",
       "      <td>185bd3a81d9d618518d10abebf0d17a8</td>\n",
       "      <td>640</td>\n",
       "      <td>360</td>\n",
       "      <td>144.0</td>\n",
       "      <td>43</td>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bd7272ed-1850-48f1-a2a8-c8fed523dc37</td>\n",
       "      <td>Ч</td>\n",
       "      <td>185bd3a81d9d618518d10abebf0d17a8</td>\n",
       "      <td>640</td>\n",
       "      <td>360</td>\n",
       "      <td>96.0</td>\n",
       "      <td>20</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          attachment_id text  ... group_rank  dataset\n",
       "0  44e8d2a0-7e01-450b-90b0-beb7400d2c1e    Ё  ...          1    train\n",
       "1  df5b08f0-41d1-4572-889c-8b893e71069b    А  ...          1    train\n",
       "2  17f53df4-c467-4aff-9f48-20687b63d49a    Р  ...          1    train\n",
       "3  e3add916-c708-4339-ad98-7e2740be29e9    Е  ...          1    train\n",
       "4  bd7272ed-1850-48f1-a2a8-c8fed523dc37    Ч  ...          1    train\n",
       "\n",
       "[5 rows x 10 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_labels = pd.read_csv(annotations_file, sep='\\t')\n",
    "video_labels['group_rank'] = video_labels.groupby(['text']).cumcount()+1;\n",
    "video_labels['dataset'] = np.where(video_labels['group_rank']<16,'train', np.where(video_labels['group_rank']<19,'val', 'test'))\n",
    "video_labels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T17:43:21.125793Z",
     "iopub.status.busy": "2023-11-21T17:43:21.124939Z",
     "iopub.status.idle": "2023-11-21T17:43:21.144551Z",
     "shell.execute_reply": "2023-11-21T17:43:21.143853Z",
     "shell.execute_reply.started": "2023-11-21T17:43:21.125756Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ё', 'А', 'Р', 'Е', 'Ч']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_list = list(video_labels['text'].unique())\n",
    "num_classes = len(labels_list)\n",
    "labels_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T17:40:16.743690Z",
     "iopub.status.busy": "2023-11-21T17:40:16.742961Z",
     "iopub.status.idle": "2023-11-21T17:40:16.772880Z",
     "shell.execute_reply": "2023-11-21T17:40:16.772227Z",
     "shell.execute_reply.started": "2023-11-21T17:40:16.743648Z"
    },
    "id": "qxL6umLs3sf6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_data = BasicVideoDataset(labels_list=labels_list, video_dir=video_dir, IMG_SIZE=IMG_SIZE, labels_df=video_labels[video_labels['dataset']=='train'])\n",
    "val_data = BasicVideoDataset(labels_list=labels_list, video_dir=video_dir, IMG_SIZE=IMG_SIZE, labels_df=video_labels[video_labels['dataset']=='val'])\n",
    "test_data = BasicVideoDataset(labels_list=labels_list, video_dir=video_dir, IMG_SIZE=IMG_SIZE, labels_df=video_labels[video_labels['dataset']=='test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T17:40:43.595220Z",
     "iopub.status.busy": "2023-11-21T17:40:43.594461Z",
     "iopub.status.idle": "2023-11-21T17:40:43.625798Z",
     "shell.execute_reply": "2023-11-21T17:40:43.625162Z",
     "shell.execute_reply.started": "2023-11-21T17:40:43.595174Z"
    },
    "id": "hnSV_-Ne1546",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T17:40:45.196188Z",
     "iopub.status.busy": "2023-11-21T17:40:45.195768Z",
     "iopub.status.idle": "2023-11-21T17:40:47.615356Z",
     "shell.execute_reply": "2023-11-21T17:40:47.614755Z",
     "shell.execute_reply.started": "2023-11-21T17:40:45.196154Z"
    },
    "id": "2tXhCmMqGyzE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "frames, label = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-11-21T17:40:47.616674Z",
     "iopub.status.busy": "2023-11-21T17:40:47.616317Z",
     "iopub.status.idle": "2023-11-21T17:40:47.640655Z",
     "shell.execute_reply": "2023-11-21T17:40:47.640060Z",
     "shell.execute_reply.started": "2023-11-21T17:40:47.616646Z"
    },
    "id": "QcDbpBd-MPaj",
    "outputId": "3b3dfd8e-eb43-4064-ed82-79d13570dcc7",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 52, 3, 224, 224])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames.shape\n",
    "# 1,x,3,244,244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-11-21T17:40:47.905505Z",
     "iopub.status.busy": "2023-11-21T17:40:47.904609Z",
     "iopub.status.idle": "2023-11-21T17:40:47.916640Z",
     "shell.execute_reply": "2023-11-21T17:40:47.916022Z",
     "shell.execute_reply.started": "2023-11-21T17:40:47.905472Z"
    },
    "id": "C0hlodXXZFuT",
    "outputId": "86b95333-cd84-4a05-9fdd-b827a05f34b5",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-11-21T17:09:16.137442Z",
     "iopub.status.busy": "2023-11-21T17:09:16.137011Z",
     "iopub.status.idle": "2023-11-21T17:09:16.154217Z",
     "shell.execute_reply": "2023-11-21T17:09:16.153446Z",
     "shell.execute_reply.started": "2023-11-21T17:09:16.137411Z"
    },
    "id": "hi5CjrTn4Ubj",
    "outputId": "68d44909-642a-427a-e5e7-bca0a95eef15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-11-21T17:09:18.768417Z",
     "iopub.status.busy": "2023-11-21T17:09:18.767830Z",
     "iopub.status.idle": "2023-11-21T17:09:18.781786Z",
     "shell.execute_reply": "2023-11-21T17:09:18.781105Z",
     "shell.execute_reply.started": "2023-11-21T17:09:18.768383Z"
    },
    "id": "XBzp4HYL4Wlz",
    "outputId": "14f94d53-875e-479e-a1e3-4f46e6676c9f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EX1m2LKowAuk"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HiYtNyjbNtb"
   },
   "source": [
    "https://programming.vip/docs/pytorch-basics-14-video-classification-based-on-pytorch.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nr7LRDHRv55x"
   },
   "source": [
    "We want to be able to train our model on a hardware accelerator like the GPU or MPS, if available. Let’s check to see if torch.cuda or torch.backends.mps are available, otherwise we use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-11-21T17:09:22.019626Z",
     "iopub.status.busy": "2023-11-21T17:09:22.018954Z",
     "iopub.status.idle": "2023-11-21T17:09:22.088907Z",
     "shell.execute_reply": "2023-11-21T17:09:22.088221Z",
     "shell.execute_reply.started": "2023-11-21T17:09:22.019586Z"
    },
    "id": "RsWcBZEhvtCc",
    "outputId": "6e91016f-8506-491e-8bea-0028cc7bf41b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "#device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHVNfZyewF4d"
   },
   "source": [
    "We define our neural network by subclassing nn.Module, and initialize the neural network layers in __init__. Every nn.Module subclass implements the operations on input data in the forward method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-11-21T17:43:27.551987Z",
     "iopub.status.busy": "2023-11-21T17:43:27.551191Z",
     "iopub.status.idle": "2023-11-21T17:43:28.358136Z",
     "shell.execute_reply": "2023-11-21T17:43:28.357449Z",
     "shell.execute_reply.started": "2023-11-21T17:43:27.551947Z"
    },
    "id": "T9VvWB81wGiQ",
    "outputId": "7fb95f9f-d291-4422-ebe0-2b7557af439e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n",
      "Resnet18Rnn(\n",
      "  (baseModel): ResNet(\n",
      "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (layer1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (layer4): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (downsample): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (fc): Identity()\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (rnn): LSTM(512, 100)\n",
      "  (fc1): Linear(in_features=100, out_features=1000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_type==\"rnn\"\n",
    "\n",
    "class Resnet18Rnn(nn.Module):\n",
    "\tdef __init__(self,params_model):\n",
    "\t\tsuper(Resnet18Rnn,self).__init__()\n",
    "\t\tnum_classes=params_model[\"num_classes\"]\n",
    "\t\tdr_rate=params_model[\"dr_rate\"]\n",
    "\t\tpretrained=params_model[\"pretrained\"]\n",
    "\t\trnn_hidden_size=params_model[\"rnn_hidden_size\"]\n",
    "\t\trnn_num_layers=params_model[\"rnn_num_layers\"]\n",
    "\t\tbaseModel=torchvision.models.resnet18(pretrained=pretrained)\n",
    "\t\tnum_features=baseModel.fc.in_features\n",
    "    # baseModel.classifier[-1]=Identity()\n",
    "\t\tbaseModel.fc=Identity() # обнуляем fully connected layer\n",
    "\t\tself.baseModel=baseModel\n",
    "\t\tself.dropout=nn.Dropout(dr_rate)\n",
    "\t\tself.rnn=nn.LSTM(num_features,rnn_hidden_size,rnn_num_layers)\n",
    "\t\tself.fc1=nn.Linear(rnn_hidden_size, num_classes)\n",
    "\tdef forward(self,x):\n",
    "\t\ttry:\n",
    "\t\t\t\tb_z,ts,c,h,w=x.shape\n",
    "\t\t\t\tii=0\n",
    "\t\t\t\ty=self.baseModel((x[:,ii]))\n",
    "\t\t\t\tout,(hn,cn)=self.rnn(y.unsqueeze(1))\n",
    "\t\t\t\tfor ii in range(1,ts):\n",
    "\t\t\t\t\ty=self.baseModel((x[:,ii]))\n",
    "\t\t\t\t\tout,(hn,cn)=self.rnn(y.unsqueeze(1),(hn,cn))\n",
    "\t\t\t\tout=self.dropout(out[:,-1])\n",
    "\t\t\t\tout=self.fc1(out)\n",
    "\t\texcept:\n",
    "\t\t\t\tprint(f'x: {x}')\n",
    "\t\t\t\tprint(f'x.shape: {x.shape}')\n",
    "\t\t\t\traise\n",
    "\t\treturn out\n",
    "\n",
    "class Identity(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(Identity,self).__init__()\n",
    "\tdef forward(self,x):\n",
    "\t\treturn x\n",
    "\n",
    "params_model={\n",
    "\t\t\"num_classes\":num_classes,\n",
    "\t\t\"dr_rate\":0.1,\n",
    "\t\t\"pretrained\":True,\n",
    "\t\t\"rnn_num_layers\":1,\n",
    "\t\t\"rnn_hidden_size\":100,\n",
    "\t\t}\n",
    "model=Resnet18Rnn(params_model)\n",
    "\n",
    "#3. Use some virtual input to test the model\n",
    "with torch.no_grad():\n",
    "\tif model_type==\"rnn\":\n",
    "\t\tx=torch.zeros(1,16,3,244,244)\n",
    "\telse:\n",
    "\t\tx=torch.zeros(1,3,16,244,244)\n",
    "\ty = model(x)\n",
    "\tprint(y.shape)\n",
    "\n",
    "#4. Move the model to the GPU device\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model=model.to(device)\n",
    "\n",
    "#5. Print model\n",
    "print(model)\n",
    "\n",
    "# According to model_type, the corresponding model will be printed. The following is the result of printing 3dcnn model:\n",
    "# VideoResNet(\n",
    "# (stem): BasicStem(\n",
    "# (0): Conv3d(3, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n",
    "# (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "# (2): ReLU(inplace=True)\n",
    "# )\n",
    "# ...\n",
    "# The printing results of rnn model are as follows:\n",
    "# Resnt18Rnn(\n",
    "# (baseModel): ResNet(\n",
    "# (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "# (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "# (relu): ReLU(inplace=True)\n",
    "# (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6bXnHS7awQLP"
   },
   "source": [
    "We create an instance of NeuralNetwork, and move it to the device, and print its structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5EtY3yGw36S"
   },
   "source": [
    "To use the model, we pass it the input data. This executes the model’s forward, along with some background operations. Do not call model.forward() directly!\n",
    "\n",
    "Calling the model on the input returns a 2-dimensional tensor with dim=0 corresponding to each output of 10 raw predicted values for each class, and dim=1 corresponding to the individual values of each output. We get the prediction probabilities by passing it through an instance of the nn.Softmax module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddoo_ipAXFNN"
   },
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T17:43:37.943943Z",
     "iopub.status.busy": "2023-11-21T17:43:37.943119Z",
     "iopub.status.idle": "2023-11-21T17:43:37.970497Z",
     "shell.execute_reply": "2023-11-21T17:43:37.969808Z",
     "shell.execute_reply.started": "2023-11-21T17:43:37.943905Z"
    },
    "id": "42djnzQ62Ngi",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import copy\n",
    "\n",
    "def get_lr(opt):\n",
    "\tfor param_group in opt.param_groups:\n",
    "\t\treturn param_group[\"lr\"]\n",
    "\n",
    "def metrics_batch(output, target):\n",
    "\tpred=output.argmax(dim=1,keepdim=True)\n",
    "\tcorrects=pred.eq(target.view_as(pred)).sum().item()\n",
    "\treturn corrects\n",
    "\n",
    "def loss_batch(loss_func, output, target, opt=None):\n",
    "\tloss=loss_func(output, target)\n",
    "\twith torch.no_grad():\n",
    "\t\tmetric_b=metrics_batch(output,target)\n",
    "\tif opt is not None:\n",
    "\t\topt.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\topt.step()\n",
    "\treturn loss.item(), metric_b\n",
    "\n",
    "def loss_epoch(model, loss_func, dataset_dl, sanity_check=False,opt=None):\n",
    "\trunning_loss=0.0\n",
    "\trunning_metric=0.0\n",
    "\tlen_data=len(dataset_dl.dataset)\n",
    "\tfor xb,yb in dataset_dl:\n",
    "\t\txb=xb.to(device)\n",
    "\t\tyb=yb.to(device)\n",
    "\t\toutput=model(xb)\n",
    "\t\tloss_b,metric_b=loss_batch(loss_func,output,yb,opt)\n",
    "\t\trunning_loss+=loss_b\n",
    "\t\tif metric_b is not None:\n",
    "\t\t\trunning_metric+=metric_b\n",
    "\t\tif sanity_check is True:\n",
    "\t\t\tbreak\n",
    "\tloss=running_loss/float(len_data)\n",
    "\tmetric=running_metric/float(len_data)\n",
    "\treturn loss, metric\n",
    "\n",
    "def plot_loss(loss_hist, metric_hist):\n",
    "\tnum_epochs=len(loss_hist[\"train\"])\n",
    "\tplt.title(\"Train-Val Loss\")\n",
    "\tplt.plot(range(1,num_epochs+1),loss_hist[\"train\"],label=\"train\")\n",
    "\tplt.plot(range(1,num_epochs+1),loss_hist[\"val\"],label=\"val\")\n",
    "\tplt.ylabel(\"Loss\")\n",
    "\tplt.xlabel(\"Training Epochs\")\n",
    "\tplt.legend()\n",
    "\tplt.show()\n",
    "\tplt.title(\"Train-Val Accuracy\")\n",
    "\tplt.plot(range(1,num_epochs+1),metric_hist[\"train\"],label=\"train\")\n",
    "\tplt.plot(range(1,num_epochs+1),metric_hist[\"val\"],label=\"val\")\n",
    "\tplt.ylabel(\"Accuracy\")\n",
    "\tplt.xlabel(\"Training Epochs\")\n",
    "\tplt.legend()\n",
    "\tplt.show()\n",
    "\n",
    "def train_val(model, params):\n",
    "\tnum_epochs=params[\"num_epochs\"]\n",
    "\tloss_func=params[\"loss_func\"]\n",
    "\topt=params[\"optimizer\"]\n",
    "\ttrain_dl=params[\"train_dl\"]\n",
    "\tval_dl=params[\"val_dl\"]\n",
    "\tsanity_check=params[\"sanity_check\"]\n",
    "\tlr_scheduler=params[\"lr_scheduler\"]\n",
    "\tpath2weights=params[\"path2weights\"]\n",
    "\n",
    "\tloss_history={\"train\":[],\"val\":[]}\n",
    "\tmetric_history={\"train\":[],\"val\":[]}\n",
    "\tbest_model_wts=copy.deepcopy(model.state_dict())\n",
    "\tbest_loss=float(\"inf\")\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tcurrent_lr=get_lr(opt)\n",
    "\t\tprint(\"Epoch {}/{}, current lr={}\".format(epoch, num_epochs-1,current_lr))\n",
    "\t\tmodel.train()\n",
    "\t\ttrain_loss,train_metric = loss_epoch(model, loss_func, train_dl, sanity_check,opt)\n",
    "\t\tloss_history[\"train\"].append(train_loss)\n",
    "\t\tmetric_history[\"train\"].append(train_metric)\n",
    "\t\tmodel.eval()\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tval_loss, val_metric = loss_epoch(model,loss_func,val_dl,sanity_check)\n",
    "\t\tif val_loss<best_loss:\n",
    "\t\t\tbest_loss=val_loss\n",
    "\t\t\tbest_model_wts=copy.deepcopy(model.state_dict())\n",
    "\t\t\ttorch.save(model.state_dict(),path2weights)\n",
    "\t\t\tprint(\"Copied best model weights\")\n",
    "\t\tloss_history[\"val\"].append(val_loss)\n",
    "\t\tmetric_history[\"val\"].append(val_metric)\n",
    "\t\tlr_scheduler.step(val_loss)\n",
    "\t\tif current_lr!=get_lr(opt):\n",
    "\t\t\tprint(\"Loading best model weights\")\n",
    "\t\t\tmodel.load_state_dict(best_model_wts)\n",
    "\t\tprint(\"Train loss:%.6f, dev loss:%.6f, accuracy:%.2f\" % (train_loss, val_loss, 100*val_metric))\n",
    "\t\tprint(\"-\"*10)\n",
    "\tmodel.load_state_dict(best_model_wts)\n",
    "\treturn model, loss_history, metric_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-21T17:43:40.261447Z",
     "iopub.status.busy": "2023-11-21T17:43:40.260807Z",
     "iopub.status.idle": "2023-11-21T17:43:40.273472Z",
     "shell.execute_reply": "2023-11-21T17:43:40.272909Z",
     "shell.execute_reply.started": "2023-11-21T17:43:40.261407Z"
    },
    "id": "hoSYMT2OXHO3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "# criterion = nn.CrossEntropyLoss().to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 852
    },
    "execution": {
     "iopub.execute_input": "2023-11-21T17:43:42.323885Z",
     "iopub.status.busy": "2023-11-21T17:43:42.322844Z",
     "iopub.status.idle": "2023-11-21T17:56:24.328638Z",
     "shell.execute_reply": "2023-11-21T17:56:24.327333Z",
     "shell.execute_reply.started": "2023-11-21T17:43:42.323822Z"
    },
    "id": "UyHmxxUB1hpE",
    "outputId": "9496b6c9-5d2e-4966-c131-bc8a90c10ca5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199, current lr=3e-05\n",
      "Copied best model weights\n",
      "Train loss:0.000440, dev loss:0.002235, accuracy:0.00\n",
      "----------\n",
      "Epoch 1/199, current lr=3e-05\n",
      "Copied best model weights\n",
      "Train loss:0.000468, dev loss:0.002161, accuracy:0.00\n",
      "----------\n",
      "Epoch 2/199, current lr=3e-05\n",
      "Train loss:0.000494, dev loss:0.002411, accuracy:0.00\n",
      "----------\n",
      "Epoch 3/199, current lr=3e-05\n",
      "Train loss:0.000447, dev loss:0.002378, accuracy:0.00\n",
      "----------\n",
      "Epoch 4/199, current lr=3e-05\n",
      "Train loss:0.000464, dev loss:0.002294, accuracy:0.00\n",
      "----------\n",
      "Epoch 5/199, current lr=3e-05\n",
      "Train loss:0.000458, dev loss:0.002370, accuracy:0.00\n",
      "----------\n",
      "Epoch 6/199, current lr=3e-05\n",
      "Train loss:0.000467, dev loss:0.002250, accuracy:0.00\n",
      "----------\n",
      "Epoch 7/199, current lr=3e-05\n",
      "Epoch 00008: reducing learning rate of group 0 to 1.5000e-05.\n",
      "Loading best model weights\n",
      "Train loss:0.000453, dev loss:0.002387, accuracy:0.00\n",
      "----------\n",
      "Epoch 8/199, current lr=1.5e-05\n",
      "Train loss:0.000463, dev loss:0.002422, accuracy:0.00\n",
      "----------\n",
      "Epoch 9/199, current lr=1.5e-05\n",
      "Train loss:0.000474, dev loss:0.002278, accuracy:0.00\n",
      "----------\n",
      "Epoch 10/199, current lr=1.5e-05\n",
      "Train loss:0.000459, dev loss:0.002429, accuracy:0.00\n",
      "----------\n",
      "Epoch 11/199, current lr=1.5e-05\n",
      "Train loss:0.000460, dev loss:0.002403, accuracy:0.00\n",
      "----------\n",
      "Epoch 12/199, current lr=1.5e-05\n",
      "Train loss:0.000450, dev loss:0.002221, accuracy:0.00\n",
      "----------\n",
      "Epoch 13/199, current lr=1.5e-05\n",
      "Epoch 00014: reducing learning rate of group 0 to 7.5000e-06.\n",
      "Loading best model weights\n",
      "Train loss:0.000486, dev loss:0.002228, accuracy:0.00\n",
      "----------\n",
      "Epoch 14/199, current lr=7.5e-06\n",
      "Train loss:0.000470, dev loss:0.002355, accuracy:0.00\n",
      "----------\n",
      "Epoch 15/199, current lr=7.5e-06\n",
      "Train loss:0.000460, dev loss:0.002390, accuracy:0.00\n",
      "----------\n",
      "Epoch 16/199, current lr=7.5e-06\n",
      "Train loss:0.000461, dev loss:0.002210, accuracy:0.00\n",
      "----------\n",
      "Epoch 17/199, current lr=7.5e-06\n",
      "Train loss:0.000460, dev loss:0.002396, accuracy:0.00\n",
      "----------\n",
      "Epoch 18/199, current lr=7.5e-06\n",
      "Train loss:0.000467, dev loss:0.002371, accuracy:0.00\n",
      "----------\n",
      "Epoch 19/199, current lr=7.5e-06\n",
      "Epoch 00020: reducing learning rate of group 0 to 3.7500e-06.\n",
      "Loading best model weights\n",
      "Train loss:0.000453, dev loss:0.002312, accuracy:0.00\n",
      "----------\n",
      "Epoch 20/199, current lr=3.75e-06\n",
      "Train loss:0.000468, dev loss:0.002411, accuracy:0.00\n",
      "----------\n",
      "Epoch 21/199, current lr=3.75e-06\n",
      "Train loss:0.000462, dev loss:0.002224, accuracy:0.00\n",
      "----------\n",
      "Epoch 22/199, current lr=3.75e-06\n",
      "Train loss:0.000484, dev loss:0.002381, accuracy:0.00\n",
      "----------\n",
      "Epoch 23/199, current lr=3.75e-06\n",
      "Train loss:0.000475, dev loss:0.002236, accuracy:0.00\n",
      "----------\n",
      "Epoch 24/199, current lr=3.75e-06\n",
      "Train loss:0.000465, dev loss:0.002294, accuracy:0.00\n",
      "----------\n",
      "Epoch 25/199, current lr=3.75e-06\n",
      "Epoch 00026: reducing learning rate of group 0 to 1.8750e-06.\n",
      "Loading best model weights\n",
      "Train loss:0.000457, dev loss:0.002165, accuracy:0.00\n",
      "----------\n",
      "Epoch 26/199, current lr=1.875e-06\n",
      "Train loss:0.000466, dev loss:0.002374, accuracy:0.00\n",
      "----------\n",
      "Epoch 27/199, current lr=1.875e-06\n",
      "Train loss:0.000474, dev loss:0.002299, accuracy:0.00\n",
      "----------\n",
      "Epoch 28/199, current lr=1.875e-06\n",
      "Train loss:0.000467, dev loss:0.002404, accuracy:0.00\n",
      "----------\n",
      "Epoch 29/199, current lr=1.875e-06\n",
      "Train loss:0.000467, dev loss:0.002175, accuracy:0.00\n",
      "----------\n",
      "Epoch 30/199, current lr=1.875e-06\n",
      "Train loss:0.000445, dev loss:0.002351, accuracy:0.00\n",
      "----------\n",
      "Epoch 31/199, current lr=1.875e-06\n",
      "Epoch 00032: reducing learning rate of group 0 to 9.3750e-07.\n",
      "Loading best model weights\n",
      "Train loss:0.000491, dev loss:0.002330, accuracy:0.00\n",
      "----------\n",
      "Epoch 32/199, current lr=9.375e-07\n",
      "Train loss:0.000484, dev loss:0.002490, accuracy:0.00\n",
      "----------\n",
      "Epoch 33/199, current lr=9.375e-07\n",
      "Train loss:0.000471, dev loss:0.002308, accuracy:0.00\n",
      "----------\n",
      "Epoch 34/199, current lr=9.375e-07\n",
      "Train loss:0.000458, dev loss:0.002372, accuracy:0.00\n",
      "----------\n",
      "Epoch 35/199, current lr=9.375e-07\n",
      "Train loss:0.000489, dev loss:0.002417, accuracy:0.00\n",
      "----------\n",
      "Epoch 36/199, current lr=9.375e-07\n",
      "Train loss:0.000454, dev loss:0.002289, accuracy:0.00\n",
      "----------\n",
      "Epoch 37/199, current lr=9.375e-07\n",
      "Epoch 00038: reducing learning rate of group 0 to 4.6875e-07.\n",
      "Loading best model weights\n",
      "Train loss:0.000472, dev loss:0.002361, accuracy:0.00\n",
      "----------\n",
      "Epoch 38/199, current lr=4.6875e-07\n",
      "Train loss:0.000487, dev loss:0.002449, accuracy:0.00\n",
      "----------\n",
      "Epoch 39/199, current lr=4.6875e-07\n",
      "Train loss:0.000475, dev loss:0.002341, accuracy:0.00\n",
      "----------\n",
      "Epoch 40/199, current lr=4.6875e-07\n",
      "Train loss:0.000460, dev loss:0.002280, accuracy:0.00\n",
      "----------\n",
      "Epoch 41/199, current lr=4.6875e-07\n",
      "Train loss:0.000483, dev loss:0.002438, accuracy:0.00\n",
      "----------\n",
      "Epoch 42/199, current lr=4.6875e-07\n",
      "Train loss:0.000450, dev loss:0.002413, accuracy:0.00\n",
      "----------\n",
      "Epoch 43/199, current lr=4.6875e-07\n",
      "Epoch 00044: reducing learning rate of group 0 to 2.3438e-07.\n",
      "Loading best model weights\n",
      "Train loss:0.000460, dev loss:0.002237, accuracy:0.00\n",
      "----------\n",
      "Epoch 44/199, current lr=2.34375e-07\n",
      "Train loss:0.000451, dev loss:0.002460, accuracy:0.00\n",
      "----------\n",
      "Epoch 45/199, current lr=2.34375e-07\n",
      "Train loss:0.000446, dev loss:0.002300, accuracy:0.00\n",
      "----------\n",
      "Epoch 46/199, current lr=2.34375e-07\n",
      "Train loss:0.000449, dev loss:0.002201, accuracy:0.00\n",
      "----------\n",
      "Epoch 47/199, current lr=2.34375e-07\n",
      "Train loss:0.000456, dev loss:0.002289, accuracy:0.00\n",
      "----------\n",
      "Epoch 48/199, current lr=2.34375e-07\n",
      "Train loss:0.000464, dev loss:0.002309, accuracy:0.00\n",
      "----------\n",
      "Epoch 49/199, current lr=2.34375e-07\n",
      "Epoch 00050: reducing learning rate of group 0 to 1.1719e-07.\n",
      "Loading best model weights\n",
      "Train loss:0.000460, dev loss:0.002177, accuracy:0.00\n",
      "----------\n",
      "Epoch 50/199, current lr=1.171875e-07\n",
      "Train loss:0.000459, dev loss:0.002196, accuracy:0.00\n",
      "----------\n",
      "Epoch 51/199, current lr=1.171875e-07\n",
      "Train loss:0.000478, dev loss:0.002255, accuracy:0.00\n",
      "----------\n",
      "Epoch 52/199, current lr=1.171875e-07\n",
      "Train loss:0.000477, dev loss:0.002229, accuracy:0.00\n",
      "----------\n",
      "Epoch 53/199, current lr=1.171875e-07\n",
      "Train loss:0.000482, dev loss:0.002223, accuracy:0.00\n",
      "----------\n",
      "Epoch 54/199, current lr=1.171875e-07\n",
      "Train loss:0.000466, dev loss:0.002286, accuracy:0.00\n",
      "----------\n",
      "Epoch 55/199, current lr=1.171875e-07\n",
      "Copied best model weights\n",
      "Train loss:0.000458, dev loss:0.002121, accuracy:0.00\n",
      "----------\n",
      "Epoch 56/199, current lr=1.171875e-07\n",
      "Train loss:0.000450, dev loss:0.002369, accuracy:0.00\n",
      "----------\n",
      "Epoch 57/199, current lr=1.171875e-07\n",
      "Train loss:0.000457, dev loss:0.002350, accuracy:0.00\n",
      "----------\n",
      "Epoch 58/199, current lr=1.171875e-07\n",
      "Train loss:0.000472, dev loss:0.002279, accuracy:0.00\n",
      "----------\n",
      "Epoch 59/199, current lr=1.171875e-07\n",
      "Train loss:0.000450, dev loss:0.002408, accuracy:0.00\n",
      "----------\n",
      "Epoch 60/199, current lr=1.171875e-07\n",
      "Train loss:0.000459, dev loss:0.002499, accuracy:0.00\n",
      "----------\n",
      "Epoch 61/199, current lr=1.171875e-07\n",
      "Epoch 00062: reducing learning rate of group 0 to 5.8594e-08.\n",
      "Loading best model weights\n",
      "Train loss:0.000483, dev loss:0.002321, accuracy:0.00\n",
      "----------\n",
      "Epoch 62/199, current lr=5.859375e-08\n",
      "Train loss:0.000471, dev loss:0.002301, accuracy:0.00\n",
      "----------\n",
      "Epoch 63/199, current lr=5.859375e-08\n",
      "Train loss:0.000448, dev loss:0.002369, accuracy:0.00\n",
      "----------\n",
      "Epoch 64/199, current lr=5.859375e-08\n",
      "Train loss:0.000454, dev loss:0.002472, accuracy:0.00\n",
      "----------\n",
      "Epoch 65/199, current lr=5.859375e-08\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-ecd058072b1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;34m\"path2weights\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"./models/weights_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodel_type\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".pt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m }\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_hist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetric_hist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m# After running the previous code snippet, the training will begin, and you should see its progress on the screen.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#3. After the training, draw the training progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-545e8cfc6f76>\u001b[0m in \u001b[0;36mtrain_val\u001b[0;34m(model, params)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msanity_check\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mbest_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                         \u001b[0mbest_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-545e8cfc6f76>\u001b[0m in \u001b[0;36mloss_epoch\u001b[0;34m(model, loss_func, dataset_dl, sanity_check, opt)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mrunning_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mlen_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset_dl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0myb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-7861c4a3a302>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mbegin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'begin'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'end'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Загрузка видео!!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-7861c4a3a302>\u001b[0m in \u001b[0;36mload_video\u001b[0;34m(self, path, begin, end, max_frames, resize)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mframe_index\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCAP_PROP_POS_FRAMES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "loss_func=nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "opt=torch.optim.Adam(model.parameters(),lr=3e-5)\n",
    "# The change of LR in cosine annealing learning rate is periodic, T_max is 1 / 2 of the period; eta_min(float) represents the minimum learning rate, which is 0 by default;\n",
    "# last_epoch(int) represents the number of previous epoch, which is used to indicate whether the learning rate needs to be adjusted. When last_ When the epoch meets the set interval,\n",
    "# The learning rate will be adjusted. When - 1, the learning rate is set to the initial value.\n",
    "# lr_scheduler = CosineAnnealingLR(opt, T_max=20, verbose=True)\n",
    "lr_scheduler=ReduceLROnPlateau(opt,mode=\"min\",factor=0.5,patience=5,verbose=1)\n",
    "os.makedirs(\"./models\",exist_ok=True)\n",
    "#2. Call train in myutils_ Val auxiliary function training model\n",
    "params_train={\n",
    "\t\"num_epochs\":200,\n",
    "\t\"optimizer\":opt,\n",
    "\t\"loss_func\":loss_func,\n",
    "\t\"train_dl\":train_dataloader,\n",
    "\t\"val_dl\":val_dataloader,\n",
    "\t\"sanity_check\":True,\n",
    "\t\"lr_scheduler\":lr_scheduler,\n",
    "\t\"path2weights\":\"./models/weights_\"+model_type+\".pt\",\n",
    "}\n",
    "model,loss_hist,metric_hist=train_val(model,params_train)\n",
    "# After running the previous code snippet, the training will begin, and you should see its progress on the screen.\n",
    "#3. After the training, draw the training progress\n",
    "plot_loss(loss_hist, metric_hist)\n",
    "# The previous clip will show a graph of loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p9C0zxcrOz76"
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5SEVU34dhWbp"
   },
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for i, (frames, labels) in enumerate(train_dataloader):\n",
    "#       frames = frames.to(device, dtype=torch.float)\n",
    "#       labels = labels.to(device, dtype=torch.float)\n",
    "#       outputs = model(frames)\n",
    "#       _, predicted = torch.max(outputs.data, 1)\n",
    "#       total += labels.size(0)\n",
    "#       correct += (predicted == labels).sum().item()\n",
    "\n",
    "# print('Test Accuracy of the model on the 10000 test images: {} %'.format((correct / total) * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "li8mPAjQk0iG"
   },
   "outputs": [],
   "source": [
    "# # Сохраняем модель и строим график\n",
    "# torch.save(model.state_dict(), MODEL_STORE_PATH + 'conv_net_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nqpefp4sk5J7"
   },
   "outputs": [],
   "source": [
    "# images.dtype"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
